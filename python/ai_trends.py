# === AI TREND DETECTOR ‚Äî Google Trends Requ√™tes Associ√©es Clone ===

from serpapi import GoogleSearch
from sentence_transformers import SentenceTransformer, util
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
import time
import json
from tqdm import tqdm
import os
from r2_uploader import upload_to_r2


# ========== CONFIGURATION ==========
FAST_REFRESH = True  # Mode rapide sans analyse d'articles
API_KEY = os.getenv("SERPAPI_KEY")
GEO_LIST = ["US", "GB", "FR", "IN", "JP", "AU"]  # zones g√©ographiques √† agr√©ger
TIME_WINDOW_HOURS = 168  # 7 jours
TOP_N = 10

# Mots √† exclure (faux positifs typiques)
BLACKLIST = [
    "fc", "football", "match", "movie", "record", "le√≥n", "music", "trailer",
    "horoscope", "festival", "concert", "wrestling", "series", "tournament"
]

# Initialisation du mod√®le s√©mantique (embeddings)
print("Loading semantic model (sentence-transformers)...")
model = SentenceTransformer("all-MiniLM-L6-v2")

# Vecteur de r√©f√©rence ‚ÄúIntelligence Artificielle‚Äù
ai_reference = model.encode(
    "artificial intelligence, machine learning, neural networks, LLM, ChatGPT, GPT, OpenAI, DeepMind, Stability AI, Mistral AI, Anthropic, AI model, AI research, Robot"
)

# ========== FONCTIONS ==========

def fetch_trends(geo):
    """R√©cup√®re les tendances SerpApi pour une r√©gion donn√©e"""
    params = {
        "engine": "google_trends_trending_now",
        "geo": geo,
        "hours": TIME_WINDOW_HOURS,
        "api_key": API_KEY
    }
    try:
        results = GoogleSearch(params).get_dict()
        return results.get("trending_searches", [])
    except Exception as e:
        print(f"Erreur API pour {geo}: {e}")
        return []

def fetch_news_snippets(news_endpoint):
    """R√©cup√®re les titres + snippets associ√©s via serpapi_news_link"""
    try:
        if not news_endpoint or "page_token=" not in news_endpoint:
            return ""
        token = news_endpoint.split("page_token=")[-1]
        results = GoogleSearch({
            "engine": "google_trends_news",
            "page_token": token,
            "api_key": API_KEY
        }).get_dict()
        articles = results.get("news_results", [])
        text = " ".join(a.get("title", "") + " " + a.get("snippet", "") for a in articles)
        return text
    except Exception:
        return ""

def semantic_score(text):
    """Calcule la similarit√© s√©mantique avec le th√®me IA"""
    if not text.strip():
        return 0.0
    vec = model.encode(text)
    return float(util.cos_sim(ai_reference, vec))

def normalize(values):
    """Normalisation 0-1"""
    arr = np.array(values, dtype=float)
    if np.ptp(arr) == 0:
        return np.ones_like(arr)
    return (arr - arr.min()) / np.ptp(arr)

def is_blacklisted(text):
    """V√©rifie si un mot interdit appara√Æt"""
    text = text.lower()
    return any(bad in text for bad in BLACKLIST)

# ========== PIPELINE PRINCIPAL ==========

print("Fetching global AI trends...")

all_trends = []
for geo in GEO_LIST:
    trends = fetch_trends(geo)
    print(f"{geo}: {len(trends)} trends r√©cup√©r√©es")
    for t in trends:
        query = t.get("query", "")
        if not query or is_blacklisted(query):
            continue
        all_trends.append({
            "query": query,
            "geo": geo,
            "search_volume": t.get("search_volume", 0) or 0,
            "news_link": t.get("serpapi_news_link", "")
        })

# D√©duplication par query
df = pd.DataFrame(all_trends).drop_duplicates(subset="query").reset_index(drop=True)

print(f"Total unique trends: {len(df)}")

# Pr√©-filtrage des mots blacklist√©s
df = df[~df["query"].str.lower().apply(is_blacklisted)].reset_index(drop=True)

# Analyse s√©mantique optimis√©e (vectoris√©e)
print("Computing semantic similarity (batched)...")
texts = df["query"].tolist()
embeddings = model.encode(texts, batch_size=64, show_progress_bar=True)
scores = util.cos_sim(ai_reference, embeddings).flatten().tolist()
df["semantic_score"] = scores
print("Semantic scoring completed!")


# Heuristique de ‚Äúfra√Æcheur‚Äù (growth_score)
df["growth_score"] = np.where(df["search_volume"] > 50000, 1,
                       np.where(df["search_volume"] > 10000, 0.8,
                       np.where(df["search_volume"] > 1000, 0.6, 0.4)))

# Normalisation
df["vol_norm"] = normalize(df["search_volume"])
df["sem_norm"] = df["semantic_score"]
df["grow_norm"] = df["growth_score"]

# üîπ Filtrage par volume minimum (ex: >= 1000 recherches)
MIN_VOLUME = 15000
df = df[df["search_volume"] >= MIN_VOLUME]

# Score final
df["score_final"] = 0.5*df["vol_norm"] + 0.3*df["grow_norm"] + 0.2*df["sem_norm"]

# Filtrage des non-IA (score < 0.35)
df = df[df["sem_norm"] >= 0.35].sort_values("score_final", ascending=False)

# Top 10 global
top10 = df.head(TOP_N)

# Affichage final
print("\nTOP 10 ‚Äî Global AI Buzz (7 derniers jours)")
for i, row in enumerate(top10.itertuples(), 1):
    print(f"{i}. {row.query}")
    print(f"   {row.geo} | Vol: {row.search_volume} | IA score: {row.semantic_score:.2f} | Final: {row.score_final:.2f}")

# Export JSON (si tu veux le stocker pour n8n)
top10.to_json("ai_trends_7days.json", orient="records", indent=2)
print("JSON generated")

print("‚òÅÔ∏è Uploading to Cloudflare R2...")
public_url = upload_to_r2(
    "ai_trends_7days.json",
    "ai_trends_7days.json"
)

print(f"Public URL: {public_url}")

