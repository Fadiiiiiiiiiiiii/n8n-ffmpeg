# === AI TREND DETECTOR ‚Äî Google Trends Requ√™tes Associ√©es Clone ===
# Author: Fadi + ChatGPT (2025)
# Description: Replicates Google Trends "related queries to AI" logic using SerpApi + semantic scoring

from serpapi import GoogleSearch
from sentence_transformers import SentenceTransformer, util
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
import time
import json
from tqdm import tqdm

# ========== CONFIGURATION ==========
FAST_REFRESH = True  # ‚ö° Mode rapide sans analyse d'articles
API_KEY = "1cbfeb224230b881918edcc85ecc545ad32a53a9bde067c5ca58f9a4ac26f9ff"  # <-- Remplace ici
GEO_LIST = ["US", "GB", "FR", "IN", "JP", "AU"]  # zones g√©ographiques √† agr√©ger
TIME_WINDOW_HOURS = 168  # 7 jours
TOP_N = 10

# Mots √† exclure (faux positifs typiques)
BLACKLIST = [
    "fc", "football", "match", "movie", "record", "le√≥n", "music", "trailer",
    "horoscope", "festival", "concert", "wrestling", "series", "tournament"
]

# Initialisation du mod√®le s√©mantique (embeddings)
print("üß† Loading semantic model (sentence-transformers)...")
model = SentenceTransformer("all-MiniLM-L6-v2")

# Vecteur de r√©f√©rence ‚ÄúIntelligence Artificielle‚Äù
ai_reference = model.encode(
    "artificial intelligence, machine learning, neural networks, LLM, ChatGPT, GPT, OpenAI, DeepMind, Stability AI, Mistral AI, Anthropic, AI model, AI research"
)

# ========== FONCTIONS ==========

def fetch_trends(geo):
    """R√©cup√®re les tendances SerpApi pour une r√©gion donn√©e"""
    params = {
        "engine": "google_trends_trending_now",
        "geo": geo,
        "hours": TIME_WINDOW_HOURS,
        "api_key": API_KEY
    }
    try:
        results = GoogleSearch(params).get_dict()
        return results.get("trending_searches", [])
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur API pour {geo}: {e}")
        return []

def fetch_news_snippets(news_endpoint):
    """R√©cup√®re les titres + snippets associ√©s via serpapi_news_link"""
    try:
        if not news_endpoint or "page_token=" not in news_endpoint:
            return ""
        token = news_endpoint.split("page_token=")[-1]
        results = GoogleSearch({
            "engine": "google_trends_news",
            "page_token": token,
            "api_key": API_KEY
        }).get_dict()
        articles = results.get("news_results", [])
        text = " ".join(a.get("title", "") + " " + a.get("snippet", "") for a in articles)
        return text
    except Exception:
        return ""

def semantic_score(text):
    """Calcule la similarit√© s√©mantique avec le th√®me IA"""
    if not text.strip():
        return 0.0
    vec = model.encode(text)
    return float(util.cos_sim(ai_reference, vec))

def normalize(values):
    """Normalisation 0-1"""
    arr = np.array(values, dtype=float)
    if np.ptp(arr) == 0:
        return np.ones_like(arr)
    return (arr - arr.min()) / np.ptp(arr)

def is_blacklisted(text):
    """V√©rifie si un mot interdit appara√Æt"""
    text = text.lower()
    return any(bad in text for bad in BLACKLIST)

# ========== PIPELINE PRINCIPAL ==========

print("üåç Fetching global AI trends...")

all_trends = []
for geo in GEO_LIST:
    trends = fetch_trends(geo)
    print(f"‚úÖ {geo}: {len(trends)} trends r√©cup√©r√©es")
    for t in trends:
        query = t.get("query", "")
        if not query or is_blacklisted(query):
            continue
        all_trends.append({
            "query": query,
            "geo": geo,
            "search_volume": t.get("search_volume", 0) or 0,
            "news_link": t.get("serpapi_news_link", "")
        })

# D√©duplication par query
df = pd.DataFrame(all_trends).drop_duplicates(subset="query").reset_index(drop=True)

print(f"üîé Total unique trends: {len(df)}")

# Analyse contextuelle
semantic_scores = []
# Analyse contextuelle (FASTREFRESH)
semantic_scores = []

for i, row in tqdm(df.iterrows(), total=len(df), desc="Analyzing context"):
    # ‚ö° Si mode rapide ‚Üí analyse uniquement la requ√™te elle-m√™me
    if FAST_REFRESH:
        text_to_analyze = row["query"]
    else:
        text_to_analyze = fetch_news_snippets(row["news_link"]) or row["query"]

    if is_blacklisted(text_to_analyze):
        semantic_scores.append(0)
        continue

    semantic_scores.append(semantic_score(text_to_analyze))

    # ‚è∏Ô∏è Supprimer le time.sleep si FAST_REFRESH
    if not FAST_REFRESH:
        time.sleep(1.5)


df["semantic_score"] = semantic_scores

# Heuristique de ‚Äúfra√Æcheur‚Äù (growth_score)
df["growth_score"] = np.where(df["search_volume"] > 50000, 1,
                       np.where(df["search_volume"] > 10000, 0.8,
                       np.where(df["search_volume"] > 1000, 0.6, 0.4)))

# Normalisation
df["vol_norm"] = normalize(df["search_volume"])
df["sem_norm"] = df["semantic_score"]
df["grow_norm"] = df["growth_score"]

# üîπ Filtrage par volume minimum (ex: >= 1000 recherches)
MIN_VOLUME = 15000
df = df[df["search_volume"] >= MIN_VOLUME]

# Score final
df["score_final"] = 0.5*df["vol_norm"] + 0.3*df["grow_norm"] + 0.2*df["sem_norm"]

# Filtrage des non-IA (score < 0.35)
df = df[df["sem_norm"] >= 0.35].sort_values("score_final", ascending=False)

# Top 10 global
top10 = df.head(TOP_N)

# Affichage final
print("\nüî• TOP 10 ‚Äî Global AI Buzz (7 derniers jours)")
for i, row in enumerate(top10.itertuples(), 1):
    print(f"{i}. {row.query}")
    print(f"   üåé {row.geo} | üìä Vol: {row.search_volume} | üß† IA score: {row.semantic_score:.2f} | ‚≠ê Final: {row.score_final:.2f}")

# Export JSON (si tu veux le stocker pour n8n)
top10.to_json("ai_trends_7days.json", orient="records", indent=2)
print("\n‚úÖ R√©sultats export√©s ‚Üí ai_trends_7days.json")
